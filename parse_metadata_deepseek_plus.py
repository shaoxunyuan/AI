#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆï¼šparse_metadata_deepseek_plus.py
åŠŸèƒ½ï¼š
- è°ƒç”¨ bioproject_info_parser.R è·å– BioProject å…ƒä¿¡æ¯
- å°†ç»“æœæ•´åˆè¿› DeepSeek promptï¼Œä»¥ä¾¿æ¨¡å‹ç†è§£é¡¹ç›®èƒŒæ™¯
- å°† BioProject ä¿¡æ¯è¿½åŠ å†™å…¥ Excel çš„ study sheet
- ä¿ç•™åŸè„šæœ¬å…¨éƒ¨åŠŸèƒ½
"""

import os
import sys
import requests
import pandas as pd
import subprocess
import json
import re
from io import StringIO

# ========== é…ç½®ä¿¡æ¯ ==========
API_KEY = os.getenv("DEEPSEEK_API_KEY")
API_URL = "https://api.deepseek.com/v1/chat/completions"

if not API_KEY:
    print("âŒ è¯·å…ˆè®¾ç½®ç¯å¢ƒå˜é‡ï¼šexport DEEPSEEK_API_KEY='ä½ çš„key'")
    sys.exit(1)


# ========== DeepSeek API è°ƒç”¨å‡½æ•° ==========
def ask_deepseek(prompt):
    headers = {
        "Authorization": f"Bearer " + API_KEY,
        "Content-Type": "application/json",
    }
    data = {
        "model": "deepseek-chat",
        "messages": [
            {"role": "system",
             "content": "ä½ æ˜¯ä¸€åèµ„æ·±ç”Ÿç‰©ä¿¡æ¯ä¸“å®¶ï¼Œæ“…é•¿è§£æ SRA metadata (pysradb) å¹¶æå–æ ·æœ¬åˆ†ç»„é€»è¾‘ã€‚"},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.2,
    }

    try:
        resp = requests.post(API_URL, headers=headers, json=data, timeout=120)
        resp.raise_for_status()
        return resp.json()["choices"][0]["message"]["content"].strip()
    except Exception as e:
        print(f"âŒ DeepSeek API è¯·æ±‚å¤±è´¥: {e}")
        sys.exit(1)


# ========== è°ƒç”¨ bioproject_info_parser.R è·å– BioProject ä¿¡æ¯ ==========
def get_bioproject_info(prj_id):
    print(f"ğŸ“˜ æ­£åœ¨è°ƒç”¨ bioproject_info_parser.R è·å– {prj_id} çš„é¡¹ç›®ä¿¡æ¯...")
    try:
        result = subprocess.run(
            ["Rscript", "bioproject_info_parser.R", prj_id],
            capture_output=True, text=True, check=True
        )
        info_text = result.stdout.strip()
        if not info_text:
            info_text = "ï¼ˆæ— è¾“å‡ºæˆ–è§£æå¤±è´¥ï¼‰"
        print("âœ… æˆåŠŸè·å– BioProject ä¿¡æ¯")
        return info_text
    except Exception as e:
        print(f"âš ï¸ è·å– BioProject ä¿¡æ¯å¤±è´¥: {e}")
        return "ï¼ˆè·å– BioProject ä¿¡æ¯å¤±è´¥ï¼‰"


# ========== ä½¿ç”¨ pysradb è·å– metadata ==========
def get_metadata_with_pysradb(prj_id):
    print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½ {prj_id} çš„ metadata...")
    try:
        cmd = ["pysradb", "metadata", prj_id, "--expand"]
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        metadata_df = pd.read_csv(StringIO(result.stdout), sep="\t", dtype=str)
        print(f"âœ… æˆåŠŸè·å– {len(metadata_df)} æ¡æ ·æœ¬è®°å½•")
        return metadata_df
    except Exception as e:
        print(f"âŒ è·å– metadata å¤±è´¥: {e}")
        return None


# ========== æ‰¹é‡è§„åˆ™åº”ç”¨å‡½æ•° ==========
def apply_grouping_rules(sample_df, rules_dict):
    sample_df = sample_df.copy()
    for col_name, rules in rules_dict.items():
        if col_name not in sample_df.columns:
            print(f"âš ï¸ è­¦å‘Š: åˆ— '{col_name}' ä¸å­˜åœ¨ï¼Œè·³è¿‡è¯¥è§„åˆ™")
            continue
        group_col_name = f"group_{col_name}"
        sample_df[group_col_name] = "NA"
        for value_pattern, group_name in rules.items():
            if value_pattern.startswith("regex:"):
                pattern = value_pattern[6:]
                mask = sample_df[col_name].str.contains(pattern, case=False, na=False)
                sample_df.loc[mask, group_col_name] = group_name
            else:
                mask = sample_df[col_name] == value_pattern
                sample_df.loc[mask, group_col_name] = group_name
    return sample_df


# ========== ä¸»æµç¨‹ ==========
def main():
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python parse_metadata_deepseek_plus.py PRJNA515702")
        sys.exit(1)

    prj_id = sys.argv[1]
    output_file = f"{prj_id}_metadata.xlsx"

    # æ£€æŸ¥ pysradb
    try:
        subprocess.run(["pysradb", "--version"], capture_output=True, check=True)
    except Exception:
        print("âŒ æœªæ‰¾åˆ° pysradbï¼Œè¯·å…ˆå®‰è£…: pip install pysradb")
        sys.exit(1)

    # Step 0ï¸âƒ£ è·å– BioProject ä¿¡æ¯
    bioproject_info_text = get_bioproject_info(prj_id)

    # Step 1ï¸âƒ£ è·å– metadata
    df = get_metadata_with_pysradb(prj_id)
    if df is None:
        sys.exit(1)
    original_df = df.copy().fillna("NA")

    # Step 2ï¸âƒ£ study/sample æ‹†åˆ†
    study_cols = [c for c in df.columns if df[c].nunique() == 1]
    sample_cols = [c for c in df.columns if df[c].nunique() > 1]
    study_df = pd.DataFrame({
        "å­—æ®µå": study_cols,
        "å­—æ®µå€¼": [df[c].iloc[0] for c in study_cols]
    })
    sample_df = df[sample_cols].fillna("NA").copy()

    # å°† BioProject ä¿¡æ¯è¿½åŠ åˆ° study_df
    study_df = pd.concat([
        study_df,
        pd.DataFrame({"å­—æ®µå": ["BioProject_Info"], "å­—æ®µå€¼": [bioproject_info_text]})
    ], ignore_index=True)

    # Step 3ï¸âƒ£ æ„é€ æç¤ºè¯
    preview_data = []
    for col in sample_cols:
        unique_vals = sample_df[col].unique()[:10]
        preview_data.append({
            "åˆ—å": col,
            "å”¯ä¸€å€¼æ•°é‡": sample_df[col].nunique(),
            "ç¤ºä¾‹å€¼": list(unique_vals)
        })
    preview_df = pd.DataFrame(preview_data)

    prompt = f"""
BioProjectå…ƒä¿¡æ¯ï¼ˆæ¥è‡ªbioproject_info_parser.Rï¼‰:
{bioproject_info_text}

ä»¥ä¸‹æ˜¯SRAæ ·æœ¬metadataçš„åˆ—ä¿¡æ¯ï¼Œè¯·ç»¼åˆåˆ†ææ ·æœ¬åˆ†ç»„é€»è¾‘ï¼š
{preview_df.to_string(index=False)}

è¯·åŸºäºè¿™äº›åˆ—çš„æ•°æ®æ¨¡å¼è¯†åˆ«åˆ†ç»„é€»è¾‘ã€‚
è¾“å‡ºè¦æ±‚ï¼š
- åªè¾“å‡ºJSONï¼Œä¸è¦é™„åŠ è§£é‡Šã€‚
- JSONç»“æ„å¦‚ä¸‹ï¼š
{{
  "grouping_columns": [
    {{
      "column_name": "åˆ—å",
      "grouping_logic": {{
        "å­—æ®µå€¼æˆ–æ¨¡å¼1": "åˆ†ç»„1",
        "å­—æ®µå€¼æˆ–æ¨¡å¼2": "åˆ†ç»„2"
      }},
      "confidence": "é«˜/ä¸­/ä½",
      "reason": "ç†ç”±"
    }}
  ]
}}
"""

    print("ğŸ¤– æ­£åœ¨è°ƒç”¨ DeepSeek è¿›è¡Œåˆ†ç»„é€»è¾‘åˆ†æ...")
    analysis_result = ask_deepseek(prompt)
    print("\n=== DeepSeek åˆ†æç»“æœ ===\n")
    print(analysis_result)
    print("========================\n")

    # Step 4ï¸âƒ£ è§£æ AI è¾“å‡º
    grouping_rules_applied = False
    rules_json = None
    try:
        json_match = re.search(r'\{.*\}', analysis_result, re.DOTALL)
        if json_match:
            rules_json = json.loads(json_match.group())
            rules_dict = {}
            for col_info in rules_json.get("grouping_columns", []):
                rules_dict[col_info["column_name"]] = col_info["grouping_logic"]
            if rules_dict:
                sample_df = apply_grouping_rules(sample_df, rules_dict)
                grouping_rules_applied = True
                print("âœ… åˆ†ç»„è§„åˆ™åº”ç”¨å®Œæˆ")
        else:
            print("âš ï¸ æœªæ£€æµ‹åˆ°æœ‰æ•ˆJSONè§„åˆ™")
    except Exception as e:
        print(f"âš ï¸ JSONè§£æå¤±è´¥: {e}")

    # Step 5ï¸âƒ£ å†™ Excel
    with pd.ExcelWriter(output_file, engine="openpyxl") as writer:
        original_df.to_excel(writer, sheet_name="metadata", index=False)
        study_df.to_excel(writer, sheet_name="study", index=False)
        sample_df.to_excel(writer, sheet_name="sample", index=False)
        if grouping_rules_applied and rules_json:
            rules_summary = [{
                "è§„åˆ™åˆ—": c["column_name"],
                "åˆ†ç»„é€»è¾‘": str(c["grouping_logic"]),
                "ç½®ä¿¡åº¦": c["confidence"],
                "è¯´æ˜": c["reason"]
            } for c in rules_json["grouping_columns"]]
            pd.DataFrame(rules_summary).to_excel(writer, sheet_name="grouping_rules", index=False)

    print(f"\nâœ… å·²ç”Ÿæˆ Excel æ–‡ä»¶ï¼š{output_file}")
    print("ğŸ“‘ åŒ…å«ä»¥ä¸‹å·¥ä½œè¡¨: metadata, study, sample, grouping_rules")


if __name__ == "__main__":
    main()
